################################
#### Mtb bwa/GATK Snakemake ####
################################
import numpy as np
from collections import defaultdict

# module load anaconda; source activate snakemake

# Define config file. Stores sample names and other things.
configfile: "config/config.yaml"

# Define samples: 
RUNS, SAMPLES = glob_wildcards(config['fastq_dir'] + "{run}/{samp}" + config['fastq1_suffix'])

# Create sample dictionary so that each sample (key) has list of runs (values) associated with it.
sample_dict = {}
for key, val in zip(SAMPLES,RUNS):
  sample_dict.setdefault(key, []).append(val)
#print(sample_dict)

# Constrain mapper and filter wildcards. 
wildcard_constraints: 
  mapper="[a-zA-Z2]+",
  filter="[a-zA-Z2]+",
  run = '|'.join([re.escape(x) for x in RUNS]),
  samp = '|'.join([re.escape(x) for x in SAMPLES]),
  ref = '|'.join([re.escape(x) for x in config['ref']])
    
# Define a rule for running the complete pipeline. 
rule all:
  input:
     trim = expand(['results/{samp}/{run}/trim/{samp}_trim_1.fq.gz'], zip, run = RUNS, samp = SAMPLES),
     kraken=expand('results/{samp}/{run}/kraken/{samp}_trim_kr_1.fq.gz', zip, run = RUNS, samp = SAMPLES),
     bams=expand('results/{samp}/{run}/bams/{samp}_{mapper}_{ref}_sorted.bam', zip, run = RUNS, samp = SAMPLES, ref = config['ref']*len(RUNS), mapper = config['mapper']*len(RUNS)), # When using zip, need to use vectors of equal lengths for all wildcards.
     per_samp_run_stats = expand('results/{samp}/{run}/stats/{samp}_{mapper}_{ref}_combined_stats.csv', zip, run = RUNS, samp = SAMPLES, ref = config['ref']*len(RUNS), mapper = config['mapper']*len(RUNS)),          
     combined_bams=expand('results/{samp}/bams/{samp}_{mapper}_{ref}.merged.rmdup.bam', samp = np.unique(SAMPLES),ref=config['ref'], mapper=config['mapper']),
     amr_stats=expand('results/{samp}/stats/{samp}_{mapper}_{ref}_amr.csv', samp=SAMPLES, ref=config['ref'], mapper=config['mapper']),
     cov_stats=expand('results/{samp}/stats/{samp}_{mapper}_{ref}_cov_stats.txt', samp=SAMPLES, ref=config['ref'], mapper=config['mapper']),
     all_sample_stats='results/stats/combined_per_run_sample_stats.csv',
     vcfs=expand('results/{samp}/vars/{samp}_{mapper}_{ref}_{caller}.vcf.gz',samp=SAMPLES, ref=config['ref'], mapper=config['mapper'], caller = config['caller']),
     ann_vcfs=expand('results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_ann.vcf.gz', samp=SAMPLES, ref=config['ref'], mapper=config['mapper'], caller = config['caller']),
     fastas=expand('results/{samp}/fasta/{samp}_{mapper}_{ref}_{caller}_{filter}.fa',samp=SAMPLES, ref=config['ref'], mapper=config['mapper'], caller = config['caller'], filter=config['filter']),
     profiles=expand('results/{samp}/stats/{samp}_{mapper}_{ref}_lineage.csv',samp=SAMPLES, ref=config['ref'], mapper=config['mapper'])
               
# Trim reads for quality. 
rule trim_reads:  
  input: 
    p1= config['fastq_dir'] + '{run}/{samp}' + config['fastq1_suffix'], 
    p2= config['fastq_dir'] + '{run}/{samp}' + config['fastq2_suffix']
  output: 
    trim1='results/{samp}/{run}/trim/{samp}_trim_1.fq.gz',
    trim2='results/{samp}/{run}/trim/{samp}_trim_2.fq.gz'
  log: 
    'results/{samp}/{run}/trim/{samp}_trim_reads.log'
  shell:
    'workflow/scripts/trim_reads.sh {input.p1} {input.p2} {output.trim1} {output.trim2} &>> {log}'

# # Filter reads taxonomically with Kraken.   
rule taxonomic_filter:
  input:
    trim1='results/{samp}/{run}/trim/{samp}_trim_1.fq.gz',
    trim2='results/{samp}/{run}/trim/{samp}_trim_2.fq.gz'
  output: 
    kr1='results/{samp}/{run}/kraken/{samp}_trim_kr_1.fq.gz',
    kr2='results/{samp}/{run}/kraken/{samp}_trim_kr_2.fq.gz',
    kraken_report='results/{samp}/{run}/kraken/{samp}_kraken.report',
    kraken_stats = 'results/{samp}/{run}/kraken/{samp}_kraken_stats.csv'
  log: 
    'results/{samp}/{run}/kraken/{samp}_kraken.log'
  threads: 8
  shell:
    'workflow/scripts/run_kraken.sh {input.trim1} {input.trim2} {output.kr1} {output.kr2} {output.kraken_report} &>> {log}'

# Map reads.
rule map_reads:
  input:
    ref_path='/labs/jandr/walter/tb/data/refs/{ref}.fa',
    kr1='results/{samp}/{run}/kraken/{samp}_trim_kr_1.fq.gz',
    kr2='results/{samp}/{run}/kraken/{samp}_trim_kr_2.fq.gz'
  output:
    bam='results/{samp}/{run}/bams/{samp}_{mapper}_{ref}_sorted.bam'
  params:
    mapper='{mapper}'
  log:
    'results/{samp}/{run}/bams/{samp}_{mapper}_{ref}_map.log'
  threads: 8
  shell:
    "workflow/scripts/map_reads.sh {input.ref_path} {params.mapper} {input.kr1} {input.kr2} {output.bam} &>> {log}"

# Get coverage & kraken statistics (per sample-run). 
rule per_samp_stats: 
  input:
    ref_path='/labs/jandr/walter/tb/data/refs/{ref}.fa',
    bam='results/{samp}/{run}/bams/{samp}_{mapper}_{ref}_sorted.bam',
    kraken_stats = 'results/{samp}/{run}/kraken/{samp}_kraken_stats.csv'
  output:
    cov_stats='results/{samp}/{run}/stats/{samp}_{mapper}_{ref}_cov_stats.txt',
    combined_stats = 'results/{samp}/{run}/stats/{samp}_{mapper}_{ref}_combined_stats.csv'
  log:
    'results/stats/{run}/{samp}_{mapper}_{ref}_cov_stats.log'
  shell:    
    '''
    workflow/scripts/cov_stats.sh {input.ref_path} {input.bam} {output.cov_stats} &>> {log}
    paste {input.kraken_stats} <(sed -n '7,8'p {output.cov_stats} ) > {output.combined_stats}
    '''
# Combine all per sample-run stats
rule all_stats:
  input: 
    combined_stats = expand('results/{samp}/{run}/stats/{samp}_{mapper}_{ref}_combined_stats.csv', zip, run = RUNS, samp = SAMPLES, ref = config['ref']*len(RUNS), mapper = config['mapper']*len(RUNS))
  output: 
    'results/stats/combined_per_run_sample_stats.csv'
  shell:
    "cat {input.combined_stats} > {output}"
  
# Combine reads and remove duplicates (per sample).
rule combine_bams:
  input:
    bams = lambda wildcards: expand('results/{samp}/{run}/bams/{samp}_{mapper}_{ref}_sorted.bam', run=sample_dict[wildcards.samp], 
        allow_missing=True)
  output: 
    combined_bam = 'results/{samp}/bams/{samp}_{mapper}_{ref}.merged.rmdup.bam'
  log: 
     'results/{samp}/bams/{samp}_{mapper}_{ref}_merge_bams.log'
  params:
    tmp_dir = 'results/{samp}/bams/'
  threads: 8
  shell:
    "sambamba markdup -r -t {threads} --tmpdir={params.tmp_dir} {input.bams} {output.combined_bam}"

# Get coverage statistics (per merged bam). 
rule cov_stats: 
  input:
    ref_path='/labs/jandr/walter/tb/data/refs/{ref}.fa',
    combined_bam = 'results/{samp}/bams/{samp}_{mapper}_{ref}.merged.rmdup.bam'
  output:
    cov_stats='results/{samp}/stats/{samp}_{mapper}_{ref}_cov_stats.txt'
  params:
    prefix='results/{samp}/stats/{samp}'
  log:
    'results/{samp}/stats/{samp}_{mapper}_{ref}_cov_stats.log'
  shell:    
    '''
    # Coverage summary statistics
    workflow/scripts/cov_stats.sh {input.ref_path} {input.combined_bam} {output.cov_stats} &>> {log}
    # Mosdepth coverage along genome (for plotting)
    mosdepth --by 2000 -Q 30 {params.prefix} {input.combined_bam}
    '''

# Run AMR prediction tool.
rule predict_amr: 
  input: 
    combined_bam = 'results/{samp}/bams/{samp}_{mapper}_{ref}.merged.rmdup.bam'
  output:
    amr_out='results/{samp}/stats/{samp}_{mapper}_{ref}_amr.csv'   
  log:
    'results/{samp}/stats/{samp}_{mapper}_{ref}_amr.log' 
  shell: 
    "workflow/scripts/mykrobe_predict.sh {input.combined_bam} {output.amr_out} &>> {log}"

# Call variants with GATK.
rule gatk_call:
  input: 
    ref_path='/labs/jandr/walter/tb/data/refs/{ref}.fa',
    combined_bam = 'results/{samp}/bams/{samp}_{mapper}_{ref}.merged.rmdup.bam'
  params:
    ploidy='1'
  output: 
    vcf='results/{samp}/vars/{samp}_{mapper}_{ref}_gatk.vcf.gz'
  log:
    'results/{samp}/vars/{samp}_{mapper}_{ref}_gatk.log' 
  shell: 
    "workflow/scripts/call_vars_gatk.sh {input.ref_path} {input.combined_bam} {params.ploidy} {output.vcf} &>> {log}"

# Annotate snps.
rule annotate_snps: 
  input:
    vcf='results/{samp}/vars/{samp}_{mapper}_{ref}_gatk.vcf.gz'
  log: 
    'results/{samp}/vars/{samp}_{mapper}_{ref}_annotate_snps.log'
  output:
    rename_vcf=temp('results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_rename.vcf.gz'),
    tmp_vcf=temp('results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_tmp.vcf.gz'),
    ann_vcf='results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_ann.vcf.gz'
  params: 
    bed=config['bed_path'],
    vcf_header=config['vcf_header']
  shell:
    '''
    # Rename Chromosome to be consistent with snpEff/Ensembl genomes.
    zcat {input.vcf}| sed 's/NC_000962.3/Chromosome/g' | bgzip > {output.rename_vcf}
    tabix {output.rename_vcf}

    # Run snpEff
    java -jar -Xmx8g {config[snpeff]} eff {config[snpeff_db]} {output.rename_vcf} -dataDir {config[snpeff_datapath]} -noStats -no-downstream -no-upstream -canon > {output.tmp_vcf}

    # Also use bed file to annotate vcf
    bcftools annotate -a {params.bed} -h {params.vcf_header} -c CHROM,FROM,TO,FORMAT/PPE {output.tmp_vcf} > {output.ann_vcf}

    '''
    
# Hard filter variants.     
rule hard_filter:
  input:
    ann_vcf='results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_ann.vcf.gz'
  log: 
    'results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_qfilt_stats.log'
  output:
    filt_vcf='results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_qfilt.vcf.gz'
  shell:
    '''
    
   # Filter VQSR
    bcftools filter -e "QUAL < 40.0 | FORMAT/DP < 10" --set-GTs '.' {input.ann_vcf} -O z -o {output.filt_vcf}

    # Index filtered VCF.
    tabix -f -p vcf {output.filt_vcf}

    # Print out stats to log file. 
    bcftools stats {output.filt_vcf} &>> {log} 
    
   '''
 
# Convert single sample VCF to fasta + filter PPE genes. 
rule vcf_to_fasta:
  input: 
    ref_path='/labs/jandr/walter/tb/data/refs/{ref}.fa',
    filt_vcf='results/{samp}/vars/{samp}_{mapper}_{ref}_gatk_qfilt.vcf.gz'
  params:
    sample_name = "{samp}",
    bed=config['bed_path']
  output:
    fasta='results/{samp}/fasta/{samp}_{mapper}_{ref}_gatk_{filt}.fa'
  log: 
   'results/{samp}/fasta/{samp}_{mapper}_{ref}_gatk_{filt}.log'
  shell:
    '''
    workflow/scripts/vcf2fasta.sh {input.ref_path} {input.filt_vcf} {params.sample_name} {params.bed} {output.fasta}    
	'''    

# TB Profiler to assign sub-lineage.
rule tb_profiler: 
  input: 
    combined_bam = 'results/{samp}/bams/{samp}_{mapper}_{ref}.merged.rmdup.bam'
  params: 
    tmp_file='results/{samp}/stats/results/{samp}.results.csv',
    outdir='results/{samp}/stats/', 
    samp='{samp}'
  output:
    profile='results/{samp}/stats/{samp}_{mapper}_{ref}_lineage.csv'   
  log:
    'results/{samp}/stats/{samp}_{mapper}_{ref}_lineage.log' 
  shell: 
    """
    tb-profiler profile --no_delly --bam {input.combined_bam} --prefix {params.samp} --dir {params.outdir} \
    --external_db /labs/jandr/walter/repos/tbdb/tbdb --csv &>> {log}
    mv {params.tmp_file} {output.profile}
    """

# Print out if success
onsuccess:
    print("Workflow finished! No error")

# Print out if workflow error
onerror:
    print("An error occurred")
